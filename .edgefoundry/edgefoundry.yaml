device: local
host: 0.0.0.0
model_path: ./models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf
port: 8000
runtime: llama_cpp
